{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 10 did not start up in 60 seconds..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "  \n",
    "\n",
    "bucket = 'retoba-reduction-etl'\n",
    "prefix = 'taketosk_common_segment/lda_spark'\n",
    "feature = 'BT'\n",
    "target_column = 'category_id'\n",
    "dir_path = 's3://{}/{}/{}'.format(bucket, prefix, feature)\n",
    "output_dir = dir_path + '/output/lda-reduction'\n",
    "\n",
    "# params for LDA\n",
    "k = 100\n",
    "maxIter = 1\n",
    "seed = 1\n",
    "optimizer = \"em\"\n",
    "\n",
    "def to_array(col):\n",
    "    def to_array_(v):\n",
    "        return v.toArray().tolist()\n",
    "    return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "  \n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('category_id', IntegerType(), False),\n",
    "    StructField('sum_value', IntegerType(), False),\n",
    "    StructField('userid', IntegerType(), False),\n",
    "])\n",
    "\n",
    "dyf = glueContext.create_dynamic_frame_from_options(\n",
    "    's3', \n",
    "    {'paths': [dir_path]}, \n",
    "    format='csv',\n",
    "    format_options={'withHeader':True})\n",
    "\n",
    "# DynamicFrame から DataFrame への変換\n",
    "df = dyf.toDF()\n",
    "df = df.withColumn(\"category_id\", f.col(\"category_id\").cast(IntegerType()))\n",
    "df = df.withColumn(\"sum_value\", f.col(\"sum_value\").cast(IntegerType()))\n",
    "df = df.withColumn(\"userid\", f.col(\"userid\").cast(IntegerType()))\n",
    "\n",
    "print(\"size of dataframe\")\n",
    "print((df.count(), len(df.columns)))\n",
    "\n",
    "# category_id で横持ちへpivot\n",
    "df2 = df.groupby('userid').pivot('category_id').sum('sum_value').fillna(0)\n",
    "print(\"pivot\")\n",
    "print((df2.count(), len(df2.columns)))\n",
    "\n",
    "# ベクトル変量への変換\n",
    "assembler = VectorAssembler(inputCols=df2.columns[1:], outputCol=\"features\")\n",
    "feature_vectors = assembler.transform(df2)\n",
    "df3 = feature_vectors[['userid', 'features']]\n",
    "print('feature vectors')\n",
    "print((df3.count(), len(df3.columns)))\n",
    "\n",
    "# LDA で次元圧縮\n",
    "# Trains a LDA model.\n",
    "model = LDA(k=k, maxIter=maxIter, seed=seed, optimizer=optimizer).fit(df3)\n",
    "score = model.transform(df3)\n",
    "df4 = score['userid','topicDistribution']\n",
    "df5 = df4.withColumn(\n",
    "    \"lda_feature\",\n",
    "    to_array(col(\"topicDistribution\"))).select([\"userid\"] + [col(\"lda_feature\")[i] for i in range(2)])\n",
    "df5.show()\n",
    "print('LDA features')\n",
    "print((df5.count(), len(df5.columns)))\n",
    "\n",
    "# 保存\n",
    "df5.write.mode(\"overwite\").csv(output_dir, header = 'true')\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
